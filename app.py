# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/138fdnDe6aduM9sPpxnbvJGLIH5gtg-96
"""

import streamlit as st
from pypdf import PdfReader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from groq import Groq

# Load Groq API key from Streamlit Secrets
groq_api_key = st.secrets["GROQ_API_KEY"]
client = Groq(api_key=groq_api_key)

# LLM Answer Function
def get_llm_answer(question, context):
    prompt = f"""
    You are an expert on the Indian Constitution.
    Use the below context to answer accurately.

    CONTEXT:
    {context}

    QUESTION:
    {question}

    ANSWER:
    """

    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}],
    )

    # FIXED LINE â†“â†“â†“
    return response.choices[0].message.content


# Load Vector DB from PDF
@st.cache_resource
def load_vector_db(pdf_file):
    reader = PdfReader(pdf_file)
    text = ""
    for page in reader.pages:
        text += page.extract_text()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = splitter.split_text(text)

    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(chunks)

    db = FAISS.from_texts(chunks, embeddings)

    return db, chunks


# Streamlit UI
st.title("ðŸ“˜ Indian Constitution QA ")

uploaded_pdf = st.file_uploader("Upload Constitution PDF", type=["pdf"])

if uploaded_pdf:
    st.success("PDF loaded! Creating Vector Storeâ€¦ (1st time only)")
    vectordb, chunks = load_vector_db(uploaded_pdf)

    question = st.text_input("Ask any question about the Constitution:")

    if question:
        docs = vectordb.similarity_search(question, k=8)
        context = "\n\n".join([d.page_content for d in docs])

        st.write("### ðŸ“Œ Answer:")
        answer = get_llm_answer(question, context)
        st.write(answer)
