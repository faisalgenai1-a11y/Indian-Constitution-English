# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/138fdnDe6aduM9sPpxnbvJGLIH5gtg-96
"""

import streamlit as st
from pypdf import PdfReader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from groq import Groq

# Load API Key from Streamlit Secrets
groq_api_key = st.secrets["GROQ_API_KEY"]
client = Groq(api_key=groq_api_key)

# -------------------------
# Function to call LLM
# -------------------------
def get_llm_answer(question, context):
    prompt = f"""
    You are an expert on the Indian Constitution.
    Use the context to answer clearly.

    CONTEXT:
    {context}

    QUESTION:
    {question}

    ANSWER:
    """

    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message["content"]


# -------------------------
# Load PDF + Create Vector DB
# -------------------------
@st.cache_resource
def load_vector_db(pdf_file):
    reader = PdfReader(pdf_file)
    full_text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            full_text += page_text

    # Split into chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = splitter.split_text(full_text)

    # Embeddings
    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vector_db = FAISS.from_texts(chunks, embedding_model)

    return vector_db, chunks


# -------------------------
# UI Starts Here
# -------------------------
st.title("ðŸ“˜ Indian Constitution QA ")

uploaded_pdf = st.file_uploader("Upload Constitution PDF", type=["pdf"])

if uploaded_pdf:
    st.success("PDF loaded! Creating Vector Storeâ€¦ (1st time only)")

    vectordb, chunks = load_vector_db(uploaded_pdf)

    question = st.text_input("Ask any question about the Constitution:")

    if question:
        docs = vectordb.similarity_search(question, k=5)
        context = "\n\n".join([d.page_content for d in docs])

        st.write("### ðŸ“Œ Answer:")
        answer = get_llm_answer(question, context)
        st.write(answer)
