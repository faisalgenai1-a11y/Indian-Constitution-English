# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/138fdnDe6aduM9sPpxnbvJGLIH5gtg-96
"""

import streamlit as st
from pypdf import PdfReader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from groq import Groq

# -----------------------------
# 1. Load Groq LLM
# -----------------------------
groq_api_key = st.secrets["GROQ_API_KEY"]
client = Groq(api_key=groq_api_key)

def get_llm_answer(question, context):
    prompt = f"""
    You are an expert on the Indian Constitution.
    Answer using the context below.

    CONTEXT:
    {context}

    QUESTION:
    {question}

    ANSWER:
    """

    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content    # IMPORTANT: correct syntax


# -----------------------------
# 2. Create Vector DB
# -----------------------------
@st.cache_resource
def load_vector_db(pdf_file):
    reader = PdfReader(pdf_file)
    text = ""
    for page in reader.pages:
        if page.extract_text():
            text += page.extract_text()

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_text(text)

    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    db = FAISS.from_texts(chunks, embedding_model)
    return db, chunks


# -----------------------------
# 3. UI
# -----------------------------
st.title("üìò Indian Constitution QA App (Groq + RAG)")

uploaded_pdf = st.file_uploader("Upload Constitution PDF", type=["pdf"])

if uploaded_pdf:

    st.success("PDF loaded! Creating Vector Store‚Ä¶ (this takes 1 minute first time)")

    vectordb, chunks = load_vector_db(uploaded_pdf)

    question = st.text_input("Ask any question about the Constitution:")

    if question:
        docs = vectordb.similarity_search(question, k=5)
        context = "\n\n".join([d.page_content for d in docs])

        st.write("### üîç Answer:")
        answer = get_llm_answer(question, context)
        st.write(answer)
